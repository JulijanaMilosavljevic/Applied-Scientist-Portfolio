# ğŸ¤— Emotion Classification with DistilBERT

This project fine-tunes the **DistilBERT** transformer model to classify English text into **six emotions**:

- ğŸ˜¢ sadness  
- ğŸ˜Š joy  
- â¤ï¸ love  
- ğŸ˜¡ anger  
- ğŸ˜¨ fear  
- ğŸ˜² surprise  

The project is lightweight, CPU-friendly, easy to reproduce, and designed as a clean, professional example of modern NLP workflow suitable for an **Applied Scientist** portfolio.
## ğŸ“ Project Structure
```bash
05-bert-emotion/
â”‚
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ emotion_bert.ipynb # Main notebook (training + evaluation)
â”‚
â”œâ”€â”€ static/
â”‚ â””â”€â”€ confusion_matrix.png # Optional: saved plots/visualizations
â”‚
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ .gitignore # Excludes model files & checkpoints
â”‚
â””â”€â”€ README.md # Project documentation
```

### ğŸ“Œ Notes

- The `models/` and `emotion-bert-results/` folders are **not included** in this repository because they contain large model files.  
- These directories are automatically generated when you run the notebook.
## âœ¨ Features

This project demonstrates a complete modern NLP workflow using Hugging Face:

### ğŸ”¹ 1. Clean & Reproducible Notebook
- No warnings  
- No unnecessary logs  
- CPU-friendly training  
- Organized step-by-step structure

### ğŸ”¹ 2. Lightweight Emotion Dataset
- Six emotion classes  
- Reduced dataset size for fast training  
- Perfect for demonstration projects

### ğŸ”¹ 3. DistilBERT Fine-Tuning
- Pretrained transformer  
- Custom classification head  
- 6-label mapping (id2label / label2id)

### ğŸ”¹ 4. Full Evaluation Suite
- Accuracy  
- Macro F1 score  
- Classification Report  
- Confusion Matrix  
- Prediction Examples  
- Error Analysis

### ğŸ”¹ 5. Clean GitHub-Ready Project
- No large files committed  
- Models are regenerated by running the notebook  
- Professional file hierarchy
## ğŸ› ï¸ Installation

To run this project locally, install the required Python libraries.

### 1ï¸âƒ£ Create a virtual environment (optional but recommended)

```bash
python -m venv venv
source venv/bin/activate   # Linux / macOS
venv\Scripts\activate      # Windows
pip install -r requirements.txt
```
## ğŸ“š Dataset Overview

This project uses the **Emotion Dataset** from Hugging Face:

**ğŸ“¦ Dataset:** `dair-ai/emotion`  
**ğŸ”— Source:** https://huggingface.co/datasets/dair-ai/emotion  

### ğŸ§© Classes (6 emotions)

| Label ID | Emotion   |
|---------|-----------|
| 0       | sadness   |
| 1       | joy       |
| 2       | love      |
| 3       | anger     |
| 4       | fear      |
| 5       | surprise  |

### ğŸª¶ Why this dataset?
- Lightweight  
- Balanced across classes  
- Perfect for fast experiments on CPU  
- Ideal for educational and portfolio projects  

### ğŸ“ Subsetting
To ensure fast training on CPU, a reduced subset is used:

- **2,000 samples** for training  
- **1,000 samples** for validation  

This keeps the project fast, reproducible, and easy for others to run.
## ğŸ·ï¸ Label Mapping

The model uses a clear mapping between numerical labels and their corresponding emotion names.
## ğŸ¤– Model & Tokenizer Setup

This project uses **DistilBERT**, a lightweight and fast transformer model ideal for fine-tuning on smaller datasets.
## âœ‚ï¸ Tokenization

Before feeding text into the model, all sentences are converted into numerical token IDs using the DistilBERT tokenizer.

### ğŸ” What happens during tokenization?

- The raw text is cleaned and normalized.
- Words are split into subword units (WordPiece).
- Special tokens are added:
  - `[CLS]` at the start  
  - `[SEP]` at the end  
- Sequences are padded or truncated to a fixed length (128 tokens).
- The output includes:
  - `input_ids`
  - `attention_mask`

### ğŸ¯ Why this matters?

Tokenization transforms human language into a format the transformer can understand.  
It ensures that:
- All input sequences have the same length  
- The model knows which tokens are real text and which are padding  
- Training and inference remain consistent and reliable
## âš™ï¸ Training Configuration

The training process is controlled using Hugging Face `TrainingArguments`, optimized specifically for CPU-friendly fine-tuning.

### ğŸ§  Key Training Parameters

- **Batch Size:** Small batch sizes are chosen to ensure memory efficiency on CPUs.
- **Epochs:** The model is trained for 2 epochs, which is sufficient for this lightweight dataset.
- **Learning Rate:** A standard transformer fine-tuning learning rate (3e-5) is used.
- **Warmup Ratio:** A warmup phase stabilizes early training steps.
- **Weight Decay:** Helps prevent overfitting.
- **Evaluation Strategy:** The model is evaluated at the end of each epoch.
- **Model Selection:** The best model checkpoint is automatically chosen based on F1 score.

### ğŸš« No GPU Required

The configuration ensures:
- Stable training on CPU-only machines  
- Reasonable training time  
- Reproducibility across environments  

This makes the project accessible to anyone who clones the repo.
## ğŸ‹ï¸ Model Training

The model is fine-tuned using the Hugging Face `Trainer` API, which manages the full training loop automatically.

### ğŸ”„ What happens during training?

- The tokenized dataset is split into training and validation sets.
- DistilBERT's pre-trained weights are updated to adapt to emotion classification.
- Loss and metric values (Accuracy, F1) are tracked for every epoch.
- Evaluation is performed at the end of each epoch to monitor progress.
- The best-performing checkpoint is saved automatically based on the highest F1 score.

### ğŸ“ˆ Why this approach?

Using the `Trainer`:
- Removes the need for a custom training loop  
- Ensures consistency and reproducibility  
- Provides built-in logging, evaluation, and checkpointing  
- Simplifies fine-tuning even for complex transformer models

This makes the project clean, efficient, and professional.
## ğŸ’¾ Saving the Model

After training, the fine-tuned DistilBERT model and its tokenizer are saved locally so they can be reused without retraining.

### ğŸ§± What gets saved?

- **Model weights** â€” the learned parameters after fine-tuning  
- **Configuration file (`config.json`)** â€” stores label mappings and model settings  
- **Tokenizer files** â€” vocabulary, merges, tokenizer settings  
- **Training outputs** â€” optional metrics and logs  

### â„¹ï¸ Important Note

These output folders can be large and are therefore **not included** in the GitHub repository.  
Instead, they are automatically recreated when the notebook is run.

This keeps the repository clean, lightweight, and professional while allowing full reproducibility.
## ğŸ” Inference

Once the model is trained and saved, it can be used to classify the emotion of any input text.

### ğŸ§  How inference works

- The text is passed through the same tokenizer used during training.
- The model predicts the probability distribution across the six emotion classes.
- The class with the highest probability is chosen as the final prediction.
- Because the model was saved with `id2label`, predictions return **human-readable emotion names** (e.g., "joy", "anger").

### ğŸ’¡ Example Use Cases

- Sentiment & emotion analysis for social media content  
- Chatbot emotion detection  
- User feedback interpretation  
- Mental health monitoring tools  

### ğŸ¯ Benefits

- Fast, lightweight predictions  
- No GPU required  
- Works on any system after loading the saved model  
## ğŸ“Š Evaluation

The model's performance is assessed using multiple metrics and visualizations that give a detailed understanding of how well it predicts each emotion.

### ğŸ§ª Key Evaluation Metrics

- **Accuracy** â€“ Measures the overall percentage of correct predictions.
- **Macro F1 Score** â€“ Averages F1 across all classes, treating all emotions equally.  
  This is especially important for imbalanced multi-class datasets.

### ğŸ“ˆ Additional Evaluation Components

#### âœ” Classification Report  
Provides precision, recall, and F1 for each of the six emotion categories.

#### âœ” Confusion Matrix  
A heatmap showing how often the model confuses one emotion with another.  
This is one of the strongest insights into model behavior and is highly valued in ML interviews.

#### âœ” Error Analysis  
A manual inspection of sample predictions to understand:
- Which emotions are easiest to classify  
- Where the model struggles  
- Why certain misclassifications happen  

### ğŸ¯ Why this matters

Strong evaluation demonstrates:
- Model reliability  
- Understanding beyond raw accuracy  
- Professional ML practice  
- Clear communication of model strengths and weaknesses


## ğŸ“‹ Training Summary Table

A clean and easy-to-read training summary table is generated at the end of the notebook.

### ğŸ§¾ What does the table show?

For each epoch, the table includes:

- **Epoch Number** â€” Indicates which training cycle the values belong to.
- **Train Loss** â€” Measures how well the model fits the training data.
- **Validation Loss** â€” Shows how well the model generalizes to unseen data.
- **Accuracy** â€” Overall classification accuracy on the validation set.
- **F1 Score** â€” Macro F1 score for a balanced evaluation across all 6 emotions.

### ğŸ¨ Why the table is useful?

- It provides a **quick visual overview** of the entire training process.
- Makes it easy to spot:
  - overfitting (train loss â‰ª val loss)  
  - underfitting (both losses high)  
  - stable learning progression  
- Enhances readability for GitHub reviewers, recruiters, and interviewers.
- Shows professionalism: clean documentation + clear metrics presentation.

### ğŸ’¡ Bonus

The table uses soft color gradients to make trends more visually intuitive and professional-looking in the notebook.

## âš ï¸ Important Project Notes

### ğŸš« Model Files Not Included in GitHub

Large model files and training outputs are **intentionally excluded** from this repository to keep it clean and lightweight.  
These directories are automatically created when running the notebook:

```bash
models/
emotion-bert-results/
```

They typically contain:
- Checkpoints  
- Tokenizer data  
- Model weights (`pytorch_model.bin`)  
- Training logs  

These files can be hundreds of megabytes in size and should **not be committed** to source control.

### ğŸ”„ Reproducibility

Anyone who clones this repository can:
1. Run the notebook  
2. Reproduce the training  
3. Regenerate all model files locally  

This ensures the project stays professional while remaining fully reproducible.

### ğŸ§¹ Why this is important?

- Keeps repository size small  
- Prevents GitHub storage bloating  
- Avoids committing binary files  
- Follows standard ML engineering best practices  
